{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lección_1_Importancia_Preparación_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh9ELGdTdmTjBTjyeWJrzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubenQuispe/Machine-Learning-con-Python-001/blob/master/Lecci%C3%B3n_1_Importancia_Preparaci%C3%B3n_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRBpr7eLp-Of"
      },
      "source": [
        "# Preparación de Datos para Machine Learning\n",
        "# MSc. Rubén Quispe\n",
        "La preparación de datos implica transformar los datos sin procesar en una forma que sea más apropiada para el modelado.\n",
        "\n",
        "La preparación de datos puede ser la parte más importante de un proyecto de modelado predictivo y la que requiere más tiempo, aunque parece ser la menos discutida. En cambio, la atención se centra en los algoritmos de aprendizaje automático, cuyo uso y parametrización se ha vuelto bastante rutinario.\n",
        "\n",
        "La preparación práctica de datos requiere conocimientos de **limpieza de datos, transformación de datos de selección de características, reducción de dimensionalidad y más**.\n",
        "\n",
        "En este curso intensivo, descubrirá cómo puede comenzar y preparar datos con confianza para un proyecto de modelado predictivo con Python en 7 lecciones.\n",
        "\n",
        "# ¿Para quién es este curso intensivo?\n",
        "Antes de comenzar, asegurémonos de que está en el lugar correcto.\n",
        "\n",
        "Este curso está dirigido a desarrolladores que pueden tener conocimientos de aprendizaje automático aplicado. Tal vez sepa cómo resolver un problema de modelado predictivo de principio a fin, o al menos la mayoría de los pasos principales, con herramientas populares.\n",
        "\n",
        "Las lecciones de este curso asumen algunas cosas sobre usted, como:\n",
        "\n",
        "Conoce Python básico para la programación.\n",
        "Es posible que conozca algunos NumPy básicos para la manipulación de matrices.\n",
        "Es posible que conozca algo de scikit-learn básico para modelar.\n",
        "NO necesita ser:\n",
        "\n",
        "¡Un mago de las matemáticas!\n",
        "¡Un experto en aprendizaje automático!\n",
        "Este curso intensivo lo llevará de un desarrollador que conoce un poco de aprendizaje automático a un desarrollador que puede preparar datos de manera efectiva y competente para un proyecto de modelado predictivo.\n",
        "\n",
        "Nota: Este curso intensivo asume que tiene un entorno Python 3 SciPy en funcionamiento con al menos NumPy instalado. Si necesita ayuda con su entorno, puede seguir el tutorial paso a paso aquí:\n",
        "\n",
        "Descripción general del curso intensivo\n",
        "Este curso intensivo se divide en siete lecciones.\n",
        "\n",
        "Puede completar una lección por día (recomendado) o completar todas las lecciones en un día (extremo). Realmente depende del tiempo que tengas disponible y de tu nivel de entusiasmo.\n",
        "\n",
        "A continuación se muestra una lista de las siete lecciones que lo ayudarán a comenzar y ser productivo con la preparación de datos en Python:\n",
        "\n",
        "* Lección 01 : Importancia de la preparación de datos\n",
        "* Lección 02 : Llene los valores perdidos con imputación (Fill missing)\n",
        "* Lección 03 : Seleccionar funciones con RFE\n",
        "* Lección 04 : Escalar datos con normalización\n",
        "* Lección 05 : Transformar categorías con codificación One-Hot\n",
        "* Lección 06 : Transformar números en categorías con kBins\n",
        "* Lección 07 : Reducción de dimensionalidad con PCA\n",
        "Cada lección puede llevarle 60 segundos o hasta 30 minutos. Tómese su tiempo y complete las lecciones a su propio ritmo. Haga preguntas e incluso publique resultados en los comentarios a continuación.\n",
        "\n",
        "Las lecciones pueden esperar que usted se vaya y descubra cómo hacer las cosas. Le daré algunas pistas, pero parte del objetivo de cada lección es obligarlo a aprender dónde buscar ayuda con los algoritmos y las mejores herramientas de Python. ( Sugerencia : tengo todas las respuestas en este blog; use el cuadro de búsqueda).\n",
        "\n",
        "Empecemos.\n",
        "\n",
        "\n",
        "# Lección 1\n",
        "# Importancia de preparación de Data\n",
        "\n",
        "\n",
        "Hola, en esta lección, descubrirás la importancia de la preparación de datos en el modelado predictivo con Machine Learning.\n",
        "\n",
        "Los proyectos de modelado predictivo implican aprender de los datos.\n",
        "\n",
        "Los datos se refieren a ejemplos o casos del dominio que caracteriza el problema que se quiere resolver.\n",
        "\n",
        "En un proyecto de modelado predictivo, como la clasificación o la regresión, los datos sin procesar normalmente no se pueden utilizar directamente.\n",
        "\n",
        "Hay cuatro razones principales por las que este es el caso:\n",
        "1. **Data Types**: los algoritmos de aprendizaje automático requieren que los datos sean números.\n",
        "2. **Data Requirements**: algunos algoritmos de aprendizaje automático imponen requisitos a los datos.  P.ej. La regresión lineal requiere que las entradas sean numéricas y no estén correlacionadas.\n",
        "3. **Data Errors**: es posible que sea necesario corregir el ruido estadístico y los errores en los datos.\n",
        "4. **Data Complexity**: las relaciones no lineales complejas pueden extraerse de los datos.\n",
        "\n",
        "Los datos sin procesar deben procesarse previamente antes de usarse para ajustar y evaluar un modelo de aprendizaje automático. Este paso en un proyecto de modelado predictivo se conoce como preparación de datos.\n",
        "\n",
        "Hay tareas comunes o estándar que puede usar o explorar durante el paso de preparación de datos en un proyecto de aprendizaje automático.\n",
        "\n",
        "Estas tareas incluyen:\n",
        "1. **Data Cleaning**: Identificar y corregir mistakes o errores en los datos.\n",
        "2. **Feature Selection**: identificación de las variables de entrada que son más relevantes para la tarea.\n",
        "3. **Data Transforms**: cambio de escala o distribución de variables.\n",
        "4. **Feature Engineering**: derivación de nuevas variables a partir de los datos disponibles.\n",
        "5. **Dimensionality Reduction**: Creación de proyecciones compactos de los datos.\n",
        "\n",
        "Cada una de estas tareas es un campo de estudio completo con algoritmos especializados.\n",
        "\n",
        "# Tu tarea\n",
        "Para esta lección, debe enumerar tres algoritmos de preparación de datos que conozca o que pueda haber usado antes y dar un resumen de una línea para su propósito.\n",
        "\n",
        "Un ejemplo de un algoritmo de preparación de datos es la **data normalization** normalización de datos que escala las variables numéricas al rango entre cero y uno.\n",
        "\n",
        "En la siguiente lección, descubrirá cómo corregir los datos que tienen valores perdidos, lo que se denomina imputación de datos.\n",
        "# Respuesta 1\n",
        "Lección 1: Tres métodos de preparación de datos\n",
        "1. Detendencia de los datos, especialmente en series de tiempo para comprender los procesos de menor escala de tiempo y relacionarlos con el forzamiento local\n",
        "2. Filtrar los datos si estamos interesados en un fenómeno de una escala temporal o espacial particular y eliminar la contribución de la varianza de otras escalas de tiempo\n",
        "3.  Reemplazo de valores no físicos con algunos valores predeterminados para eliminar el sesgo de los mismos\n",
        "\n",
        "## Respuesta 2\n",
        "En una aplicación, tengo 5 sensores, 4 ángulos de medición con un potenciómetro angular y una respuesta lineal con un potenciómetro lineal. Para permitir la homogeneidad entre los datos, utilicé los datos convertidos de analógico a digital y normalicé cada valor entre -1 y 1 (tengo alarma de umbrales negativos y positivos para manejar). Bajé el ruido integrando 100 muestras por segundo (cada sensor por separado). Utilicé un valor de predicción estadística de Bayes para desviar datos después de una colección de 128 datos por sensor, la desviación media y estándar calculada y hacer la predicción con los últimos datos entrantes.\n",
        "## respuesta 3\n",
        "Lección 1: Tres métodos de preparación de datos\n",
        "\n",
        "1. **Numerical data discretization** Discretización de datos numéricos: transforme datos numéricos en datos categóricos. Esto puede resultar útil cuando los rangos pueden ser más efectivos que los valores exactos en el proceso de modelado. Por ejemplo: las temperaturas altas-medias-bajas pueden ser más interesantes que la temperatura real.\n",
        "\n",
        "2. **Outlier Detection** Detección de valores atípicos: al utilizar el diagrama de caja (boxplot), es posible identificar valores que pueden estar fuera del rango que podríamos esperar. Los valores atípicos pueden ser ruidos y, por lo tanto, no ayudan al proceso de búsqueda de patrones en conjuntos de datos.\n",
        "\n",
        "3. **Creation of new attribute** Creación de un nuevo atributo: al combinar los atributos existentes, podría ser interesante crear un nuevo atributo que pueda ayudar en el proceso de modelado. Por ejemplo: rango de temperatura basado en temperatura mínima y máxima.\n",
        " # respuesta\n",
        "Tres algoritmos que he usado para el preprocesamiento de datos\n",
        "* **Data standarization** Estandarización de datos que estandariza los datos numéricos utilizando la media y la desviación estándar de la columna.\n",
        "\n",
        "También he usado **Correlation plot** gráficos de correlación para identificar las correlaciones y, junto con eso, he usado VIF para identificar columnas interdependientes.\n",
        "\n",
        "Aparte de eso, he usado buscar y reemplazar simples para reemplazar valores basura o nulos en los datos con valores medios, modales o estáticos.\n",
        "\n",
        "\n",
        "# Lección: preparación de datos y algunas técnicas\n",
        "\n",
        "La preparación de datos es el proceso de limpieza y transformación de datos sin procesar o raw data antes de procesarlos y analizarlos.\n",
        "\n",
        "Una buena preparación de datos permite un análisis eficiente, limita los errores, reduce las anomalías y las inexactitudes que pueden ocurrir durante el procesamiento de datos y hace que todos los datos procesados ​​sean accesibles para los usuarios.\n",
        "\n",
        "# Algunas otras técnicas:\n",
        "\n",
        "1.  **Data wrangling/Cleaning** Reorganización o negociación de datos / limpieza de datos: es el proceso de limpieza y unificación de datos desordenados y complejos para un fácil acceso y análisis. Esto implica completar los valores faltantes y deshacerse de los valores atípicos en el conjunto de datos.\n",
        "\n",
        "2. **Data Discretization** Discretización de datos: parte de la reducción de datos pero de particular importancia, especialmente para los datos numéricos. En esta rutina, los valores brutos del atributo numérico se colocan mediante etiquetas de intervalo (contenedores) o etiquetas conceptuales.\n",
        "3. **Data reduction** Reducción de datos - Obtiene una representación reducida en volumen pero coloca resultados analíticos iguales o similares. Algunas de esas técnicas son el filtro de alta correlación, PCA, árboles de decisión / bosque aleatorio y eliminación de características hacia atrás / adelante.\n",
        "¿Cuál será su consideración si se le pregunta cuáles son las diferentes técnicas de ingeniería de características?\n",
        "\n",
        "\n",
        "Le preguntaría al interrogador qué quieren decir con ingeniería de funciones. Sugeriría que las características polinomiales son un método de ingeniería de características, también esto ayudará:\n",
        "[feature engineering](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
        "\n",
        "Tengo la siguiente consulta.\n",
        "\n",
        "Para los modelos de pronóstico de regresión, clasificación y series de tiempo, encontramos términos como R2, Accuracy_Score, MSE, RMSE, AIC, BIC para evaluar el rendimiento del modelo (puede hacerme saber si me perdí alguna otra métrica aquí)\n",
        "\n",
        "¿Cuántas de las métricas de precisión anteriores deben usarse para cualquier modelo? ¿Qué combinación de ellos se utilizará? ¿Depende del modelo?\n",
        "Pick one metric and optimize it.\n",
        "\n",
        "# Lección 1:\n",
        "\n",
        "Estos son los algoritmos que encontré, recién estoy comenzando en el aprendizaje automático \n",
        "\n",
        "1. **Independent Componet Analysis** Análisis de componentes independientes: se utiliza para separar una combinación compleja de datos en sus diferentes fuentes.\n",
        "\n",
        "2. **Principal component analysis** Análisis de componentes principales (PCA): se utiliza para reducir la dimensionalidad de los datos mediante la creación de nuevas funciones. Hace esto para aumentar sus posibilidades de ser interpretables mientras minimiza la pérdida de información.\n",
        "\n",
        "3. **Forwar blackward feature seection** Selección de funciones hacia adelante / hacia atrás: también se utiliza para reducir la cantidad de funciones en un conjunto de datos, sin embargo, a diferencia de PCA, no crea nuevas funciones.\n",
        "gracias Ruben !\n",
        "\n",
        "# Algoritmos de preparación de datos:\n",
        "1. **Normalization** Normalización: para normalizar los rangos de columnas numéricas en la escala para reducir la diferencia entre rangos.\n",
        "\n",
        "2. **Standarization** Estandarización: para estandarizar los valores de entrada numéricos con la media y la desviación estándar para reducir las diferencias entre los valores.\n",
        "\n",
        "3. **NominalToBinary**: Para convertir valores nominales en valores binarios.\n",
        "\n",
        "# Respuesta de la lección 1:\n",
        "\n",
        "Tratando de pensar en cosas que aún no se han mencionado y que no forman parte de los capítulos posteriores de este curso, así que aquí están mis pensamientos:\n",
        "\n",
        "1. Limpiar datos para combinar múltiples instancias de lo que es esencialmente la misma observación, con diferentes grafías, por ejemplo. como el nombre del cliente Mike, Michael o M, todos relacionados con el mismo ID de cliente. Esto nos da una imagen más real de los valores de un atributo para cada observación. Por ejemplo, si estamos midiendo la lealtad del cliente por el número de compras de un cliente determinado, necesitamos todos esos registros de Michael combinados en uno.\n",
        "\n",
        "2. Pivotear datos en forma larga eliminando atributos de campo separados para lo que debería ser un campo (por ejemplo, años 2018, 2019, 2020 como atributos separados en lugar de pivotar en un solo atributo “Año”). Esto permite un análisis más fácil del conjunto de datos.\n",
        "\n",
        "3. Eliminar caracteristicas con un valor constante en todos los registros. Estos no añaden valor al modelo predictivo.\n",
        "\n",
        "Enumere tres algoritmos de preparación de datos que conozca o pueda haber usado antes y proporcione un resumen de una línea para su propósito.\n",
        "\n",
        "1. Análisis de componentes principales\n",
        "Reduce la dimensionalidad del conjunto de datos creando nuevas características que se correlacionan con más de una característica original.\n",
        "\n",
        "2. Conjuntos de árboles de decisión\n",
        "Utilizado para la selección de funciones\n",
        "\n",
        "3. Selección de funciones hacia adelante y selección de funciones hacia atrás\n",
        "Se aplica para reducir el número de funciones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPNnyF9q1MeQ",
        "outputId": "ee422725-221d-4c50-f263-f3940f225466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "[“’40-49′” “‘premeno'” “’15-19′” “‘0-2′” “‘yes'” “‘3′” “‘right'”\n",
        "“‘left_up'” “‘no'”]\n",
        "[“’50-59′” “‘ge40′” “’15-19′” “‘0-2′” “‘no'” “‘1′” “‘right'” “‘central'”\n",
        "“‘no'”]\n",
        "[“’50-59′” “‘ge40′” “’35-39′” “‘0-2′” “‘no'” “‘2′” “‘left'” “‘left_low'”\n",
        "“‘no'”]\n",
        "\n",
        "After transform:\n",
        "\n",
        "[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
        "0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
        "[0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
        "0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
        "[0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
        "0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6095a91f7b9f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [“’40-49′” “‘premeno'” “’15-19′” “‘0-2′” “‘yes'” “‘3′” “‘right'”\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK9gj97H7o9R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}